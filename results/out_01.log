None
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

INFRASTRUCTURE -> torch

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:28:55.931134', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:01.743454', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:07.613434', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:13.305358', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:19.575849', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:25.173583', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:30.778851', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:36.319150', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:42.247579', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:47.854549', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:53.444603', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:29:58.919254', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:04.923789', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:10.381729', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:15.834529', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:21.291264', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:27.340187', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:32.816306', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:38.307709', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:43.748601', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:50.363921', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:54.602354', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:30:58.883503', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:03.190681', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:07.495186', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:11.808855', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:16.340653', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:20.830879', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:25.062461', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:29.686276', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:33.956182', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:38.454813', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:42.695233', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:47.152670', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:51.464159', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:55.720054', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:31:59.987139', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:04.285701', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:08.577766', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:12.820282', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:20.760297', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:25.765403', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:30.720221', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:35.831123', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:40.836396', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:45.812272', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:50.749607', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:32:55.810424', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:00.971112', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:05.910708', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:10.876169', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:15.867866', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:21.231731', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:26.186098', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:31.114237', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:36.164205', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:41.232737', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:46.221527', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:51.183248', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:33:56.142417', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:02.750904', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:06.929770', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:11.051695', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:15.185843', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:19.381150', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:23.483152', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:27.586610', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:31.773523', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:35.932518', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:40.016784', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:44.378151', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:48.545205', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:52.704322', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:34:56.794347', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:00.876990', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:05.052799', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:09.259319', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:13.600047', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:17.765946', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:21.950556', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:34.268430', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:42.034084', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:35:49.543196', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:36:02.687598', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:36:17.680042', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:36:29.708147', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:36:37.448489', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:36:45.228897', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:36:52.922837', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:00.484091', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:07.998822', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:15.731974', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:23.383988', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:30.894084', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:38.435461', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:46.130041', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:37:53.748241', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:38:01.249823', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:38:08.785376', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:38:16.431324', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
------------------------------

