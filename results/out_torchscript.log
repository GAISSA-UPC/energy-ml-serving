None
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

INFRASTRUCTURE -> torchscript

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt, reps -> 1

------------------------------

---------------- Rep 1 out of 1: 

Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:59:49.566680', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :_world_ return,'}}}
Inference successful!
{'prediction': ' : :_world_ return,'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:59:52.935377', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i i i i()'}}}
Inference successful!
{'prediction': ' i i i i()'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:59:56.560888', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : print ", world!")))),'}}}
Inference successful!
{'prediction': ' : print ", world!")))),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T00:59:59.901789', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : i i 5'}}}
Inference successful!
{'prediction': ' : i i 5'}


CodeCarbon Results: results/emissions_codet5-base.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt, reps -> 1

------------------------------

---------------- Rep 1 out of 1: 

Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:08.451101', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'defHelloprint_##'}}}
Inference successful!
{'prediction': 'defHelloprint_##'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:11.856048', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'EnumI in rangeEnum))):'}}}
Inference successful!
{'prediction': 'EnumI in rangeEnum))):'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:15.200302', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(print(""""!"printprint'}}}
Inference successful!
{'prediction': '(print(""""!"printprint'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:18.523703', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'varxEnumEnum:'}}}
Inference successful!
{'prediction': 'varxEnumEnum:'}


CodeCarbon Results: results/emissions_codet5p-220.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt, reps -> 1

------------------------------

---------------- Rep 1 out of 1: 

Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:24.782463', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'ers_world\n\n'}}}
Inference successful!
{'prediction': 'ers_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:27.407457', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' example = 1(1):\n'}}}
Inference successful!
{'prediction': ' example = 1(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:30.032008', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(\'\\ World world!");\n'}}}
Inference successful!
{'prediction': '(\'\\ World world!");\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:32.618532', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( = 0)\n'}}}
Inference successful!
{'prediction': ' ( = 0)\n'}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt, reps -> 1

------------------------------

---------------- Rep 1 out of 1: 

Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:38.069370', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' test(world(\n   '}}}
Inference successful!
{'prediction': ' test(world(\n   '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:40.720287', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '_ in range(len):\n           '}}}
Inference successful!
{'prediction': '_ in range(len):\n           '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:43.305811', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '("  , world!")\n       '}}}
Inference successful!
{'prediction': '("  , world!")\n       '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:45.871819', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' not. 0:\n       '}}}
Inference successful!
{'prediction': ' not. 0:\n       '}


CodeCarbon Results: results/emissions_codeparrot-small.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt, reps -> 1

------------------------------

---------------- Rep 1 out of 1: 

Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:00:58.937955', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': ':_world\n\n'}}}
Inference successful!
{'prediction': ':_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:01:03.129041', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' ( in range(1):\n'}}}
Inference successful!
{'prediction': ' ( in range(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:01:07.127658', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '([, world!")\n'}}}
Inference successful!
{'prediction': '([, world!")\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-13T01:01:11.110891', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( < 0 &&\n'}}}
Inference successful!
{'prediction': ' ( < 0 &&\n'}


CodeCarbon Results: results/emissions_pythia-410m.csv
------------------------------

