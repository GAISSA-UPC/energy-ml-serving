None
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

INFRASTRUCTURE -> torch

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:27:05.992930', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:27:16.842707', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:27:27.643499', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:27:37.333103', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:27:47.233321', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:27:53.257566', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:27:59.030656', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:04.802289', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:11.103660', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:16.881701', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:22.657956', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:29.527189', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:35.745423', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:41.504322', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:47.208665', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:52.881293', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:28:59.080732', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:04.936107', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:10.652647', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:16.410412', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torch', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:22.555381', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:27.184987', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:31.776711', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:36.360349', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:40.993481', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:45.599277', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:50.200532', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:54.997452', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:29:59.559015', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:04.387053', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:09.030204', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:13.639615', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:18.214565', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:22.886143', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:27.464492', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:32.006829', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:36.666966', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:41.284746', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:45.826275', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:50.355582', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torch', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:30:56.012130', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:01.109002', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:06.263410', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:11.525436', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:16.864382', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:22.288620', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:27.559241', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:32.856129', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:38.237763', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:43.659520', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:48.850430', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:54.180856', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:31:59.529926', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:04.688705', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:09.965844', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:15.330511', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:20.529632', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:25.629104', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:30.764050', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:35.951568', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torch', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:41.038943', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:45.384481', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:49.695241', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:54.081009', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:32:58.412763', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:02.759684', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:07.177555', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:11.468173', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:15.806069', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:20.057792', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:24.353672', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:28.733218', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:33.040194', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:37.309859', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:41.620427', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:45.999324', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:50.297320', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:54.583521', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:33:58.859374', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:03.173626', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torch', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:11.831928', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:19.585006', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:27.355591', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:35.221407', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:43.117645', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:50.809463', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:34:58.525620', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:35:06.577624', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:35:14.580371', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:35:22.408962', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:35:30.266594', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:35:38.154817', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:35:46.308237', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:35:54.081163', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:36:01.946029', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:36:10.010941', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torch, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:36:18.243353', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:36:25.957886', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:36:33.854514', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torch
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:36:41.633572', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torch', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
------------------------------

None
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

INFRASTRUCTURE -> onnx

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> onnx, model -> codet5-base, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:38:51.440508', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:38:57.894821', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i ii'}}}
Inference successful!
{'prediction': ' i ii'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:03.025956', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:08.134654', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:13.417773', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:18.415023', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i ii'}}}
Inference successful!
{'prediction': ' i ii'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:23.655919', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:30.087982', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:41.423869', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:39:52.514785', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i ii'}}}
Inference successful!
{'prediction': ' i ii'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:01.572107', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:07.471052', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:12.574023', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:17.568101', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i ii'}}}
Inference successful!
{'prediction': ' i ii'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:22.843403', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:27.964643', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:33.063555', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:38.044582', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i ii'}}}
Inference successful!
{'prediction': ' i ii'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:43.277952', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:48.513389', 'url': 'http://localhost:8000/huggingface_models/codet5-base/onnx', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : :::::'}}}
Inference successful!
{'prediction': ' : :::::'}


CodeCarbon Results: results/emissions_codet5-base.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> onnx, model -> codet5p-220, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:40:59.753302', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:05.309508', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:10.807534', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:16.271875', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:21.715104', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:27.185711', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:33.046028', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:38.508828', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:44.344577', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:49.821733', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:41:55.697621', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:01.127819', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:06.681340', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:12.201413', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:18.092074', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:23.961378', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:29.483662', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:34.970345', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:40.414076', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:45.854966', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/onnx', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}


CodeCarbon Results: results/emissions_codet5p-220.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> onnx, model -> gpt-neo-125m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:42:54.809951', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:00.646517', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:06.520450', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:12.537579', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:18.673119', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:24.526035', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:30.367023', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:36.291677', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:42.443946', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:48.270317', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:43:54.328635', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:00.200483', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:06.370681', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:12.258936', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:18.353991', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:24.283957', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:30.164461', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:35.981959', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:41.795777', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:47.958963', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/onnx', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> onnx, model -> codeparrot-small, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:44:56.052539', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:02.286839', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:09.600977', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:15.154949', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:20.681243', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:26.437619', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:31.947640', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:37.732833', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:43.544926', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:49.259295', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:45:54.821740', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:00.562363', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:06.405315', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:11.969383', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:17.513800', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:23.107932', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:28.937804', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:34.561850', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:40.320178', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:46:45.918002', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/onnx', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> onnx, model -> pythia-410m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 0 out of 5: 

Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:47:07.922443', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:47:19.676875', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:47:31.427435', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:47:43.319961', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:47:55.928152', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:48:07.897578', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:48:19.850714', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:48:31.858562', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:48:43.758847', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:48:56.154373', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:49:11.671711', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:49:25.904552', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:49:37.885029', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:49:49.675078', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:50:05.768823', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:50:28.490882', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> onnx, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:50:42.536026', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:50:54.787489', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:51:07.060478', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/onnx
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:51:19.483811', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/onnx', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
------------------------------

None
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

INFRASTRUCTURE -> ov

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> ov, model -> codet5-base, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:52:29.141863', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:52:37.248481', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:52:42.390620', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:52:47.475644', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:52:53.213281', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:52:58.313942', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:03.413657', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:08.515587', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:14.258999', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:19.349526', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:24.482217', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:29.595265', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:35.348473', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:40.465233', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:45.600351', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:50.720198', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:53:56.465210', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world(self, message) : self.hello_world(message'}}}
Inference successful!
{'prediction': 'def hello_world(self, message) : self.hello_world(message'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:01.592773', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 't h i s.'}}}
Inference successful!
{'prediction': 't h i s.'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:06.777316', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 't r y { c'}}}
Inference successful!
{'prediction': 't r y { c'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:11.937317', 'url': 'http://localhost:8000/huggingface_models/codet5-base/ov', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'c ase x : c'}}}
Inference successful!
{'prediction': 'c ase x : c'}


CodeCarbon Results: results/emissions_codet5-base.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> ov, model -> codet5p-220, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:28.027626', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:33.833579', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:39.485500', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:45.125081', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:50.784387', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:54:56.413949', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:02.096681', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:07.796663', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:13.441139', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:19.086463', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:24.741312', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:30.387064', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:36.029481', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:41.674071', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:47.334690', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:53.006281', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:55:58.675235', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}}}
Inference successful!
{'prediction': '\n    print "Hello world!"\n\ndef main():\n    print "Hello world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:04.363682', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}}}
Inference successful!
{'prediction': '\n# Copyright (c) 2012-2014 The Bitcoin developers\n# Distributed under the'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:10.045274', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}}}
Inference successful!
{'prediction': '\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:15.694461', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/ov', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}}}
Inference successful!
{'prediction': '\n#include "../include/c_stdlib.h"\n#include "'}


CodeCarbon Results: results/emissions_codet5p-220.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> ov, model -> gpt-neo-125m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:24.669573', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:28.192930', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:31.662223', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:35.185407', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:38.697070', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:42.138101', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:45.588211', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:49.109913', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:52.637512', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:56.094606', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:56:59.557669', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:03.072315', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:06.591193', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:10.056745', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:13.516565', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:17.031131', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:20.547210', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n              '}}}
Inference successful!
{'prediction': 'def hello_world():\n              '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:24.012573', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            '}}}
Inference successful!
{'prediction': 'for i in range(10):\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:27.474438', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n            '}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n            '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:31.002723', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/ov', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n              '}}}
Inference successful!
{'prediction': 'if x > 5:\n              '}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> ov, model -> codeparrot-small, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:38.684482', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:44.198734', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:49.439575', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:57:54.803605', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:00.056616', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:05.283985', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:10.641451', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:16.161058', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:21.468769', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:26.671951', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:31.886816', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:37.323246', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:42.672431', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:47.970836', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:53.174319', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:58:58.619002', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:59:03.968471', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}}}
Inference successful!
{'prediction': 'def hello_world():\n    return "Hello World!"\n\ndef hello_world_with_'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:59:09.227945', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}}}
Inference successful!
{'prediction': 'for i in range(10):\n            self.assertEqual(self.get_response(i),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:59:14.534109', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n        self.assertEqual(self.client.get("/hello/world'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:59:19.769762', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/ov', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}}}
Inference successful!
{'prediction': 'if x > 5:\n        return False\n    if x < 10:\n        return False\n    if x'}


CodeCarbon Results: results/emissions_codeparrot-small.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> ov, model -> pythia-410m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T19:59:47.142333', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:00:14.680098', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:00:32.566337', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:00:43.918779', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:00:53.978375', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:01:03.831296', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:01:13.586358', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:01:23.242275', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:01:32.861799', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:01:42.260900', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:01:51.700541', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:02:01.365808', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:02:11.138779', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:02:20.715132', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:02:30.281635', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:02:39.891483', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> ov, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:02:49.518104', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}}}
Inference successful!
{'prediction': 'def hello_world():\n        print("Hello World")\n\nif __name__ == "'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:02:58.956238', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}}}
Inference successful!
{'prediction': 'for i in range(10):\n        if i % 10 == 0:\n            print('}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:03:08.469169', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}}}
Inference successful!
{'prediction': 'print("Hello, world!")\n\nA:\n\nYou can use the following code:'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/ov
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:03:18.087038', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/ov', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}}}
Inference successful!
{'prediction': 'if x > 5:\n            print("You have reached the maximum number of characters in the file'}


CodeCarbon Results: results/emissions_pythia-410m.csv
------------------------------

None
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

INFRASTRUCTURE -> torchscript

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:03:47.736285', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :_world_ return,'}}}
Inference successful!
{'prediction': ' : :_world_ return,'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:03:51.335277', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i i i i()'}}}
Inference successful!
{'prediction': ' i i i i()'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:03:54.830462', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : print ", world!")))),'}}}
Inference successful!
{'prediction': ' : print ", world!")))),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:03:58.344108', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : i i 5'}}}
Inference successful!
{'prediction': ' : i i 5'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:01.880562', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :_world_ return,'}}}
Inference successful!
{'prediction': ' : :_world_ return,'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:05.438008', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i i i i()'}}}
Inference successful!
{'prediction': ' i i i i()'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:08.934878', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : print ", world!")))),'}}}
Inference successful!
{'prediction': ' : print ", world!")))),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:12.435923', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : i i 5'}}}
Inference successful!
{'prediction': ' : i i 5'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:15.936136', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :_world_ return,'}}}
Inference successful!
{'prediction': ' : :_world_ return,'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:19.459439', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i i i i()'}}}
Inference successful!
{'prediction': ' i i i i()'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:22.956717', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : print ", world!")))),'}}}
Inference successful!
{'prediction': ' : print ", world!")))),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:26.466557', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : i i 5'}}}
Inference successful!
{'prediction': ' : i i 5'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:29.933730', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :_world_ return,'}}}
Inference successful!
{'prediction': ' : :_world_ return,'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:33.408427', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i i i i()'}}}
Inference successful!
{'prediction': ' i i i i()'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:36.926188', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : print ", world!")))),'}}}
Inference successful!
{'prediction': ' : print ", world!")))),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:40.429636', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : i i 5'}}}
Inference successful!
{'prediction': ' : i i 5'}


CodeCarbon Results: results/emissions_codet5-base.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5-base, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:44.689179', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' : :_world_ return,'}}}
Inference successful!
{'prediction': ' : :_world_ return,'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:48.181129', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' i i i i()'}}}
Inference successful!
{'prediction': ' i i i i()'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:51.709477', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': ' : print ", world!")))),'}}}
Inference successful!
{'prediction': ' : print ", world!")))),'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5-base/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:04:55.202765', 'url': 'http://localhost:8000/huggingface_models/codet5-base/torchscript', 'data': {'model-type': 'Codet5_base', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' : i i 5'}}}
Inference successful!
{'prediction': ' : i i 5'}


CodeCarbon Results: results/emissions_codet5-base.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:03.843376', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'defHelloprint_##'}}}
Inference successful!
{'prediction': 'defHelloprint_##'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:07.434104', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'EnumI in rangeEnum))):'}}}
Inference successful!
{'prediction': 'EnumI in rangeEnum))):'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:10.932125', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(print(""""!"printprint'}}}
Inference successful!
{'prediction': '(print(""""!"printprint'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:14.405057', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'varxEnumEnum:'}}}
Inference successful!
{'prediction': 'varxEnumEnum:'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:17.849631', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'defHelloprint_##'}}}
Inference successful!
{'prediction': 'defHelloprint_##'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:21.323225', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'EnumI in rangeEnum))):'}}}
Inference successful!
{'prediction': 'EnumI in rangeEnum))):'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:24.808413', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(print(""""!"printprint'}}}
Inference successful!
{'prediction': '(print(""""!"printprint'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:28.278760', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'varxEnumEnum:'}}}
Inference successful!
{'prediction': 'varxEnumEnum:'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:31.735266', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'defHelloprint_##'}}}
Inference successful!
{'prediction': 'defHelloprint_##'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:35.238570', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'EnumI in rangeEnum))):'}}}
Inference successful!
{'prediction': 'EnumI in rangeEnum))):'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:38.711752', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(print(""""!"printprint'}}}
Inference successful!
{'prediction': '(print(""""!"printprint'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:42.190603', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'varxEnumEnum:'}}}
Inference successful!
{'prediction': 'varxEnumEnum:'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:45.644988', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'defHelloprint_##'}}}
Inference successful!
{'prediction': 'defHelloprint_##'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:49.366301', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'EnumI in rangeEnum))):'}}}
Inference successful!
{'prediction': 'EnumI in rangeEnum))):'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:52.855873', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(print(""""!"printprint'}}}
Inference successful!
{'prediction': '(print(""""!"printprint'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:56.335937', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'varxEnumEnum:'}}}
Inference successful!
{'prediction': 'varxEnumEnum:'}


CodeCarbon Results: results/emissions_codet5p-220.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codet5p-220, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:05:59.795508', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'defHelloprint_##'}}}
Inference successful!
{'prediction': 'defHelloprint_##'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:03.283005', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': 'EnumI in rangeEnum))):'}}}
Inference successful!
{'prediction': 'EnumI in rangeEnum))):'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:06.786210', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(print(""""!"printprint'}}}
Inference successful!
{'prediction': '(print(""""!"printprint'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codet5p-220/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:10.282077', 'url': 'http://localhost:8000/huggingface_models/codet5p-220/torchscript', 'data': {'model-type': 'Codet5p_220m', 'input_text': 'if x > 5:', 'prediction': {'prediction': 'varxEnumEnum:'}}}
Inference successful!
{'prediction': 'varxEnumEnum:'}


CodeCarbon Results: results/emissions_codet5p-220.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:16.724524', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'ers_world\n\n'}}}
Inference successful!
{'prediction': 'ers_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:19.485153', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' example = 1(1):\n'}}}
Inference successful!
{'prediction': ' example = 1(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:22.227116', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(\'\\ World world!");\n'}}}
Inference successful!
{'prediction': '(\'\\ World world!");\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:24.944710', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( = 0)\n'}}}
Inference successful!
{'prediction': ' ( = 0)\n'}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:27.671466', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'ers_world\n\n'}}}
Inference successful!
{'prediction': 'ers_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:30.380682', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' example = 1(1):\n'}}}
Inference successful!
{'prediction': ' example = 1(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:33.112251', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(\'\\ World world!");\n'}}}
Inference successful!
{'prediction': '(\'\\ World world!");\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:35.801624', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( = 0)\n'}}}
Inference successful!
{'prediction': ' ( = 0)\n'}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:38.487450', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'ers_world\n\n'}}}
Inference successful!
{'prediction': 'ers_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:41.210581', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' example = 1(1):\n'}}}
Inference successful!
{'prediction': ' example = 1(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:43.934072', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(\'\\ World world!");\n'}}}
Inference successful!
{'prediction': '(\'\\ World world!");\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:46.623192', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( = 0)\n'}}}
Inference successful!
{'prediction': ' ( = 0)\n'}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:49.307139', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'ers_world\n\n'}}}
Inference successful!
{'prediction': 'ers_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:52.016047', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' example = 1(1):\n'}}}
Inference successful!
{'prediction': ' example = 1(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:54.722725', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(\'\\ World world!");\n'}}}
Inference successful!
{'prediction': '(\'\\ World world!");\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:06:57.414491', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( = 0)\n'}}}
Inference successful!
{'prediction': ' ( = 0)\n'}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> gpt-neo-125m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:00.101356', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'def hello_world():', 'prediction': {'prediction': 'ers_world\n\n'}}}
Inference successful!
{'prediction': 'ers_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:02.813987', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' example = 1(1):\n'}}}
Inference successful!
{'prediction': ' example = 1(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:05.505240', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '(\'\\ World world!");\n'}}}
Inference successful!
{'prediction': '(\'\\ World world!");\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:08.221184', 'url': 'http://localhost:8000/huggingface_models/gpt-neo-125m/torchscript', 'data': {'model-type': 'GPT_Neo_125m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( = 0)\n'}}}
Inference successful!
{'prediction': ' ( = 0)\n'}


CodeCarbon Results: results/emissions_gpt-neo-125m.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:13.774076', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' test(world(\n   '}}}
Inference successful!
{'prediction': ' test(world(\n   '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:16.482374', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '_ in range(len):\n           '}}}
Inference successful!
{'prediction': '_ in range(len):\n           '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:19.162552', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '("  , world!")\n       '}}}
Inference successful!
{'prediction': '("  , world!")\n       '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:21.838490', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' not. 0:\n       '}}}
Inference successful!
{'prediction': ' not. 0:\n       '}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:24.516596', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' test(world(\n   '}}}
Inference successful!
{'prediction': ' test(world(\n   '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:27.183491', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '_ in range(len):\n           '}}}
Inference successful!
{'prediction': '_ in range(len):\n           '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:29.842770', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '("  , world!")\n       '}}}
Inference successful!
{'prediction': '("  , world!")\n       '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:32.511545', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' not. 0:\n       '}}}
Inference successful!
{'prediction': ' not. 0:\n       '}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:35.188338', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' test(world(\n   '}}}
Inference successful!
{'prediction': ' test(world(\n   '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:37.874660', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '_ in range(len):\n           '}}}
Inference successful!
{'prediction': '_ in range(len):\n           '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:40.525716', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '("  , world!")\n       '}}}
Inference successful!
{'prediction': '("  , world!")\n       '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:43.190658', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' not. 0:\n       '}}}
Inference successful!
{'prediction': ' not. 0:\n       '}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:45.834986', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' test(world(\n   '}}}
Inference successful!
{'prediction': ' test(world(\n   '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:48.517134', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '_ in range(len):\n           '}}}
Inference successful!
{'prediction': '_ in range(len):\n           '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:51.180199', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '("  , world!")\n       '}}}
Inference successful!
{'prediction': '("  , world!")\n       '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:53.842750', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' not. 0:\n       '}}}
Inference successful!
{'prediction': ' not. 0:\n       '}


CodeCarbon Results: results/emissions_codeparrot-small.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> codeparrot-small, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:56.502085', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'def hello_world():', 'prediction': {'prediction': ' test(world(\n   '}}}
Inference successful!
{'prediction': ' test(world(\n   '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:07:59.165665', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'for i in range(10):', 'prediction': {'prediction': '_ in range(len):\n           '}}}
Inference successful!
{'prediction': '_ in range(len):\n           '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:01.865906', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '("  , world!")\n       '}}}
Inference successful!
{'prediction': '("  , world!")\n       '}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/codeparrot-small/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:04.552384', 'url': 'http://localhost:8000/huggingface_models/codeparrot-small/torchscript', 'data': {'model-type': 'CodeParrot_small', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' not. 0:\n       '}}}
Inference successful!
{'prediction': ' not. 0:\n       '}


CodeCarbon Results: results/emissions_codeparrot-small.csv
------------------------------

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running experiment infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt, reps -> 5

------------------------------

---------------- Rep 1 out of 5: 

Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:17.652170', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': ':_world\n\n'}}}
Inference successful!
{'prediction': ':_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:21.763551', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' ( in range(1):\n'}}}
Inference successful!
{'prediction': ' ( in range(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:25.739410', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '([, world!")\n'}}}
Inference successful!
{'prediction': '([, world!")\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:29.641397', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( < 0 &&\n'}}}
Inference successful!
{'prediction': ' ( < 0 &&\n'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 2 out of 5: 

Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:33.528118', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': ':_world\n\n'}}}
Inference successful!
{'prediction': ':_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:37.433116', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' ( in range(1):\n'}}}
Inference successful!
{'prediction': ' ( in range(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:41.314037', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '([, world!")\n'}}}
Inference successful!
{'prediction': '([, world!")\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:45.195307', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( < 0 &&\n'}}}
Inference successful!
{'prediction': ' ( < 0 &&\n'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 3 out of 5: 

Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:49.050921', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': ':_world\n\n'}}}
Inference successful!
{'prediction': ':_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:52.949491', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' ( in range(1):\n'}}}
Inference successful!
{'prediction': ' ( in range(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:08:56.835140', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '([, world!")\n'}}}
Inference successful!
{'prediction': '([, world!")\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:00.723459', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( < 0 &&\n'}}}
Inference successful!
{'prediction': ' ( < 0 &&\n'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 4 out of 5: 

Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:04.623169', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': ':_world\n\n'}}}
Inference successful!
{'prediction': ':_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:08.536646', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' ( in range(1):\n'}}}
Inference successful!
{'prediction': ' ( in range(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:12.443682', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '([, world!")\n'}}}
Inference successful!
{'prediction': '([, world!")\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:16.603870', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( < 0 &&\n'}}}
Inference successful!
{'prediction': ' ( < 0 &&\n'}


CodeCarbon Results: results/emissions_pythia-410m.csv
---------------- Rep 5 out of 5: 

Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> testing/inputs.txt

Dataset: ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']
---
Running POST requests infrastructure -> torchscript, model -> pythia-410m, dataset -> ['def hello_world():', 'for i in range(10):', 'print("Hello, world!")', 'if x > 5:']

___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:20.465718', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'def hello_world():', 'prediction': {'prediction': ':_world\n\n'}}}
Inference successful!
{'prediction': ':_world\n\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:24.313224', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'for i in range(10):', 'prediction': {'prediction': ' ( in range(1):\n'}}}
Inference successful!
{'prediction': ' ( in range(1):\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:28.175851', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'print("Hello, world!")', 'prediction': {'prediction': '([, world!")\n'}}}
Inference successful!
{'prediction': '([, world!")\n'}
___________________________________
Endpoint: http://localhost:8000/huggingface_models/pythia-410m/torchscript
{'message': 'OK', 'method': 'POST', 'status-code': 200, 'timestamp': '2023-12-03T20:09:32.031686', 'url': 'http://localhost:8000/huggingface_models/pythia-410m/torchscript', 'data': {'model-type': 'Pythia_410m', 'input_text': 'if x > 5:', 'prediction': {'prediction': ' ( < 0 &&\n'}}}
Inference successful!
{'prediction': ' ( < 0 &&\n'}


CodeCarbon Results: results/emissions_pythia-410m.csv
------------------------------

