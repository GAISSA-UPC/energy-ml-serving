{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install codecarbon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd8eb2",
   "metadata": {},
   "source": [
    "Use this notebook in the Sagemaker jupyter instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39940f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69fd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = '/home/fjdur/cloud-api/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c417d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::150660304444:role/sagamaker_role\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagamaker_role')['Role']['Arn']\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cd4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: Salesforce/codet5p-220m\n"
     ]
    }
   ],
   "source": [
    "models = [ 'codet5-base', 'codet5p-220', 'codegen-350-mono', 'gpt-neo-125m', 'codeparrot-small', 'pythia-410m'] # bloom, pythia\n",
    "model_checkpoint = {'codet5-base':\"Salesforce/codet5-base\", 'codet5p-220':'Salesforce/codet5p-220m', \n",
    "                    'codegen-350-mono':\"Salesforce/codegen-350M-mono\", 'gpt-neo-125m':\"EleutherAI/gpt-neo-125M\",\n",
    "                    'codeparrot-small':'codeparrot/codeparrot-small', 'pythia-410m':\"EleutherAI/pythia-410m\"} # model:checkpoint\n",
    "\n",
    "model_name = models[1]\n",
    "checkpoint = model_checkpoint[model_name]\n",
    "print(f'checkpoint: {checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9853bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.huggingface.model.HuggingFaceModel object at 0x7fdc3b3b0310>\n"
     ]
    }
   ],
   "source": [
    "# Code sagemaker.huggingface\n",
    "#https://github.com/aws/sagemaker-python-sdk/blob/c3a5fb01827fdd2cdad66a2b659a2a9a574153a2/src/sagemaker/huggingface/model.py\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID' : checkpoint, # model_id from hf.co/models\n",
    "  'HF_TASK' : 'text-generation' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.26\", # transformers version used\n",
    "   pytorch_version=\"1.13\", # pytorch version used\n",
    "   py_version=\"py39\", # python version of the DLC\n",
    ")\n",
    "print(huggingface_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550128c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-pytorch-inference-2023-08-22-12-36-01-988'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a92801a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d3db166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "predictor_reuse=sagemaker.predictor.Predictor(\n",
    "    endpoint_name=\"huggingface-pytorch-inference-2023-08-22-12-01-20-881\",\n",
    "    #sagemaker_session=sagemaker.Session(),\n",
    "    #serializer=sagemaker.serializers.CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ab5389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this predictor to reuse the same endpoint and instance than the notebook\n",
    "predictor_reuse2=sagemaker.huggingface.model.HuggingFacePredictor(\n",
    "    endpoint_name=\"huggingface-pytorch-inference-2023-08-22-12-01-20-881\",\n",
    "    #sagemaker_session=sagemaker.Session(),\n",
    "    #serializer=sagemaker.serializers.CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3418b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7fdc3996dde0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5470ddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7fdc333b03a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_reuse2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "426518b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "\"inputs\":  \"def hello_world():\"\n",
    "}\n",
    "\n",
    "# request\n",
    "from codecarbon import track_emissions\n",
    "\n",
    "#response = predictor.predict(data)\n",
    "\n",
    "#@track_emissions(project_name = \"codet5p-220_sm\", output_file = RESULTS_DIR + \"emissions_codet5p-220.csv\")\n",
    "@track_emissions(project_name = \"codet5p-220_sm\",output_file = \"emissions_codet5p-220.csv\")\n",
    "def infer(predictor, data):\n",
    "    return predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606dd678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:23:01] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:23:01] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:23:01] No GPU found.\n",
      "[codecarbon INFO @ 12:23:01] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:23:01] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 12:23:01] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 12:23:01] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:23:01]   Platform system: Linux-5.10.178-162.673.amzn2.x86_64-x86_64-with-glibc2.26\n",
      "[codecarbon INFO @ 12:23:01]   Python version: 3.10.10\n",
      "[codecarbon INFO @ 12:23:01]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 12:23:01]   Available RAM : 15.325 GB\n",
      "[codecarbon INFO @ 12:23:01]   CPU count: 4\n",
      "[codecarbon INFO @ 12:23:01]   CPU model: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 12:23:01]   GPU count: None\n",
      "[codecarbon INFO @ 12:23:01]   GPU model: None\n",
      "[codecarbon INFO @ 12:23:03] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 12:23:03] Energy consumed for RAM : 0.000002 kWh. RAM Power : 5.746757984161377 W\n",
      "[codecarbon INFO @ 12:23:03] Energy consumed for all CPUs : 0.000037 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 12:23:03] 0.000039 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:23:03] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'def hello_world():def hello_world_world()def'}]\n"
     ]
    }
   ],
   "source": [
    "response = infer(predictor, data)\n",
    "\n",
    "print(response)\n",
    "\n",
    "#predictor.delete_model()\n",
    "#predictor.delete_endpoint()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22b22850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 12:45:22] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 12:45:22] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 12:45:22] No GPU found.\n",
      "[codecarbon INFO @ 12:45:22] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 12:45:22] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 12:45:23] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 12:45:23] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 12:45:23]   Platform system: Linux-5.10.178-162.673.amzn2.x86_64-x86_64-with-glibc2.26\n",
      "[codecarbon INFO @ 12:45:23]   Python version: 3.10.10\n",
      "[codecarbon INFO @ 12:45:23]   CodeCarbon version: 2.3.1\n",
      "[codecarbon INFO @ 12:45:23]   Available RAM : 15.325 GB\n",
      "[codecarbon INFO @ 12:45:23]   CPU count: 4\n",
      "[codecarbon INFO @ 12:45:23]   CPU model: Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz\n",
      "[codecarbon INFO @ 12:45:23]   GPU count: None\n",
      "[codecarbon INFO @ 12:45:23]   GPU model: None\n",
      "[codecarbon INFO @ 12:45:24] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 12:45:24] Energy consumed for RAM : 0.000002 kWh. RAM Power : 5.746757984161377 W\n",
      "[codecarbon INFO @ 12:45:24] Energy consumed for all CPUs : 0.000033 kWh. Total CPU Power : 105.0 W\n",
      "[codecarbon INFO @ 12:45:24] 0.000034 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 12:45:24] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'def hello_world():def hello_world_world()def'}]\n"
     ]
    }
   ],
   "source": [
    "response = infer(predictor_reuse2, data)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c548aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
